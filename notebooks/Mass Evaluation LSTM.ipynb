{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import time\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Notebook helper methods\n",
    "from bella import notebook_helper\n",
    "# Models\n",
    "from bella.models.tdlstm import LSTM, TDLSTM, TCLSTM\n",
    "# Tokenisers\n",
    "from bella.tokenisers import ark_twokenize\n",
    "# Word Vectors\n",
    "from bella.word_vectors import PreTrained, GloveCommonCrawl\n",
    "# Get the data\n",
    "from bella.parsers import semeval_14, dong, election\n",
    "from bella.data_types import TargetCollection\n",
    "from bella.helper import read_config, full_path\n",
    "from bella.evaluation import evaluation_results\n",
    "from bella.notebook_helper import get_json_data, write_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "mitchel_train = semeval_14(full_path(read_config('mitchel_train')))\n",
    "mitchel_test = semeval_14(full_path(read_config('mitchel_test')))\n",
    "\n",
    "\n",
    "dataset_train_test = {'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      'Mitchel' : (mitchel_train, mitchel_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "# Word vectors that we are searching over\n",
    "word_vectors = [sswe, glove_300]\n",
    "\n",
    "\n",
    "# This is required as we have 3 classes and one of them is -1 and when one hot encoded\n",
    "# the index of -1 is 2 and that is what it thinks the label is when it should be \n",
    "# -1 hence the sentiment mapper\n",
    "sentiment_mapper = {0 : 0, 1 : 1, 2 : -1}\n",
    "\n",
    "# Folder to store all the sub folder for each model\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDLstm'))\n",
    "# Folder to store all of the saved models (model zoo folder)\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo'))\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model_class, \n",
    "                        word_vector_file_path, result_file_path,\n",
    "                        model_folder_path, model_params):\n",
    "    \n",
    "    print('{} {}'.format(dataset_name, model_params))\n",
    "\n",
    "    data_train = train.data_dict()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data_dict()\n",
    "    y_test = test.sentiment_data()\n",
    "    \n",
    "    # Fits the model\n",
    "    word_vector_data = get_json_data(word_vector_file_path, dataset_name)\n",
    "    best_score = 0\n",
    "    best_word_vector = None\n",
    "    best_model = None\n",
    "    for word_vector in word_vectors:\n",
    "        print(word_vector)\n",
    "        word_vector_name = '{}'.format(word_vector)\n",
    "        if word_vector_name in word_vector_data:\n",
    "            word_vec_val_score = word_vector_data[word_vector_name]\n",
    "            if word_vec_val_score > best_score:\n",
    "                best_score = word_vec_val_score\n",
    "                best_word_vector = word_vector\n",
    "            continue\n",
    "        model_params['embeddings'] = word_vector\n",
    "        model = model_class(**model_params)\n",
    "        print('{} {}'.format(model_params, word_vector))\n",
    "        history = model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n",
    "                            reproducible=True, patience=10, epochs=300, org_initialisers=True)\n",
    "        word_vec_val_score = max(history.history['val_acc'])\n",
    "        word_vector_data[word_vector_name] = word_vec_val_score\n",
    "        if word_vec_val_score > best_score:\n",
    "                best_score = word_vec_val_score\n",
    "                best_word_vector = word_vector\n",
    "                best_model = model\n",
    "                \n",
    "        # Save word vector validation score result\n",
    "        write_json_data(word_vector_file_path, dataset_name, word_vector_data)\n",
    "    if best_word_vector is None:\n",
    "        raise ValueError('best word vector should not be None')\n",
    "    if best_model is None:\n",
    "        model_params['embeddings'] = best_word_vector\n",
    "        model = model_class(**model_params)\n",
    "        print('{} {}'.format(model_params, best_word_vector))\n",
    "        model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n",
    "                  reproducible=True, patience=10, epochs=300, org_initialisers=True)\n",
    "    # Saves the model to the model zoo\n",
    "    model_folder_join = lambda file_name: os.path.join(model_folder_path, file_name)\n",
    "    model_arch_fp = model_folder_join('{} {} architecture'.format(model, dataset_name))\n",
    "    model_weights_fp = model_folder_join('{} {} weights'.format(model, dataset_name))\n",
    "    model.save_model(model_arch_fp, model_weights_fp, verbose=1)\n",
    "    \n",
    "    # Predicts on the test data\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Convert prediction from one hot encoded to category value e.g. -1, 0, 1\n",
    "    predicted_values_cats =  model.prediction_to_cats(y_test, predicted_values, \n",
    "                                                      mapper=sentiment_mapper)\n",
    "    # Evaluates the predictions and save the results\n",
    "    return evaluation_results(predicted_values_cats, test, dataset_name, \n",
    "                              file_name=result_file_path, \n",
    "                              save_raw_data=True, re_write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model folder results\n",
    "lstm_folder = os.path.join(result_folder, 'lstm')\n",
    "os.makedirs(lstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(lstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(lstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, LSTM, \n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the TDLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model folder results\n",
    "tdlstm_folder = os.path.join(result_folder, 'tdlstm')\n",
    "os.makedirs(tdlstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(tdlstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(tdlstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, TDLSTM,\n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the TCLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model folder results\n",
    "tclstm_folder = os.path.join(result_folder, 'tclstm')\n",
    "os.makedirs(tclstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(tclstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(tclstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, TCLSTM, \n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
